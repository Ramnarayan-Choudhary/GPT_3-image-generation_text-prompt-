{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nD1n0xEBcko"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4iTie2EKrbb",
        "outputId": "473ba92e-f4f9-4658-e3a8-b028595aa254"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unable to create process using 'C:\\Users\\Ram Naryan\\anaconda3\\New folder\\python.exe \"C:\\Users\\Ram Naryan\\anaconda3\\New folder\\Scripts\\pip-script.py\" install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html'\n",
            "Unable to create process using 'C:\\Users\\Ram Naryan\\anaconda3\\New folder\\python.exe \"C:\\Users\\Ram Naryan\\anaconda3\\New folder\\Scripts\\pip-script.py\" install kornia==0.5.0'\n",
            "fatal: destination path 'CLIP' already exists and is not an empty directory."
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] The system cannot find the path specified: '/content/CLIP/'\n",
            "c:\\Users\\Ram Naryan\\Documents\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Unable to create process using 'C:\\Users\\Ram Naryan\\anaconda3\\New folder\\python.exe \"C:\\Users\\Ram Naryan\\anaconda3\\New folder\\Scripts\\pip-script.py\" install ftfy regex'\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'clip'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\RAMNAR~1\\AppData\\Local\\Temp/ipykernel_15664/3612946379.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# 4.Modeling CLIP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' pip install ftfy regex'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ViT-B/32'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'clip'"
          ]
        }
      ],
      "source": [
        "# Pytorch version change\n",
        "! pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# . Install Pytorch image processing library\n",
        "! pip install kornia==0.5.0\n",
        "\n",
        "# Copy of CLIP related code\n",
        "! git clone https://github.com/openai/CLIP.git\n",
        "%cd /content/CLIP/\n",
        "\n",
        "# 4.Modeling CLIP\n",
        "! pip install ftfy regex\n",
        "import clip\n",
        "model, preprocess = clip.load('ViT-B/32', jit=False)\n",
        "model = model.eval()\n",
        "# 5. Modeling DALL-E\n",
        "! pip install DALL-E\n",
        "from dall_e import map_pixels, unmap_pixels, load_model\n",
        "dec = load_model(\"https://cdn.openai.com/dall-e/decoder.pkl\", 'cuda')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAcixx9Z3XYH"
      },
      "source": [
        "# library imports & function definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piJOg9MY7khd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as TF\n",
        "import torchvision.transforms as T\n",
        "import kornia\n",
        "import PIL\n",
        "import os, io, sys\n",
        "import random\n",
        "import imageio\n",
        "from IPython import display\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "from google.colab import output\n",
        "import requests\n",
        "\n",
        "# Initial setting\n",
        "im_shape = [512, 512, 3]\n",
        "sideX, sideY, channels = im_shape\n",
        "target_image_size = sideX\n",
        "tau_value = 2.\n",
        "# Display/save image\n",
        "def displ(img):\n",
        "  img = np.array(img)[:,:,:]\n",
        "  img = np.transpose(img, (1, 2, 0))\n",
        "  imageio.imwrite('output.png', np.array(img))\n",
        "  return display.Image('output.png')\n",
        "\n",
        "# Random image clipping\n",
        "def augment(out, cutn=16):\n",
        "  p_s = []\n",
        "  for ch in range(cutn):\n",
        "    sizey = int(torch.zeros(1,).uniform_(.5, .99)*sideY)\n",
        "    sizex = int(torch.zeros(1,).uniform_(.5, .99)*sideX)\n",
        "    offsetx = torch.randint(0, sideX - sizex, ())\n",
        "    offsety = torch.randint(0, sideY - sizey, ())\n",
        "    apper = out[:, :, offsetx:offsetx + sizex, offsety:offsety + sizey]\n",
        "    apper = apper + .1*torch.rand(1,1,1,1).cuda()*torch.randn_like(apper, requires_grad=True)\n",
        "    apper = torch.nn.functional.interpolate(apper, (224,224), mode='bilinear')\n",
        "    p_s.append(apper)\n",
        "  into = augs(torch.cat(p_s, 0))\n",
        "  return into\n",
        "\n",
        "\n",
        "# normalization and rotation settings\n",
        "nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "augs = kornia.augmentation.RandomRotation(30).cuda()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaocGDQXz3Zx"
      },
      "source": [
        "# extract feature vector from text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGBTOiJqWgZ3"
      },
      "outputs": [],
      "source": [
        "# input text\n",
        "text_input = 'a beautiful and mysterious house designed by Escher'\n",
        "# convert text to feature vector\n",
        "token = clip.tokenize(text_input)\n",
        "text_v = model.encode_text(token.cuda()).detach().clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSqoQrpGCUp0",
        "outputId": "9f50eeeb-95eb-4877-e172-6e11dca22728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "token.shape =  torch.Size([1, 77])\n",
            "token =  tensor([[49406,   320,  1215,   537, 12650,  1212,  4486,   638,    68, 21299,\n",
            "         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0]], dtype=torch.int32)\n",
            "text_v.shape =  torch.Size([1, 512])\n"
          ]
        }
      ],
      "source": [
        "# [Check] shape of token, text_v\n",
        "print('token.shape = ', token.shape)\n",
        "print('token = ', token)\n",
        "print('text_v.shape = ', text_v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiAD3aRNMC4l"
      },
      "source": [
        "# Setting parameters and optimization methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdCh2D8Dt8Xd"
      },
      "outputs": [],
      "source": [
        "# set parameters\n",
        "class Pars(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pars, self).__init__()\n",
        "        hots = torch.nn.functional.one_hot((torch.arange(0, 8192).to(torch.int64)), num_classes=8192)\n",
        "        rng = torch.zeros(1, 64*64, 8192).uniform_()\n",
        "        for i in range(64*64):\n",
        "            rng[0,i] = hots[[np.random.randint(8191)]]\n",
        "        rng = rng.permute(0, 2, 1)\n",
        "        self.normu = torch.nn.Parameter(rng.cuda().view(1, 8192, 64*64))\n",
        "\n",
        "    def forward(self):\n",
        "      normu = torch.nn.functional.gumbel_softmax(self.normu.reshape(1,64*64,8192), dim=1, tau=tau_value).view(1, 8192, 64, 64)\n",
        "      return normu\n",
        "# set optimization method\n",
        "latent = Pars().cuda()\n",
        "param = [latent.normu]\n",
        "optimizer = torch.optim.Adam([{'params': param, 'lr': .01}])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZyFGu6IC5Rx"
      },
      "outputs": [],
      "source": [
        "# [Check] Image generation from parameters\n",
        "with torch.no_grad():\n",
        "  out = unmap_pixels(torch.sigmoid(dec(latent())[:, :3].float()))\n",
        "  displ(out.cpu()[0])\n",
        "\n",
        "  print('latent().shape = ', latent().shape)\n",
        "  print('dec(latent()).shape = ', dec(latent()).shape)\n",
        "  print('out.shape = ', out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WztSrRF23Rqg"
      },
      "source": [
        "# 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwYNUzzovPEW"
      },
      "outputs": [],
      "source": [
        "# 学習ループ\n",
        "for iteration in range(1001):\n",
        "\n",
        "  # --- 順伝播 ---\n",
        "  # パラメータから画像を生成\n",
        "  out = unmap_pixels(torch.sigmoid(dec(latent())[:, :3].float()))\n",
        "  # 画像をランダム切り出し・回転\n",
        "  into = augment(out)\n",
        "  # 画像を正規化\n",
        "  into = nom((into))\n",
        "  # 画像から特徴ベクトルを取得\n",
        "  image_v = model.encode_image(into)\n",
        "  # テキストと画像の特徴ベクトルのCOS類似度を計算\n",
        "  loss = -torch.cosine_similarity(text_v, image_v).mean()\n",
        "\n",
        "  # 逆伝播\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # 学習率の調整\n",
        "  for g in optimizer.param_groups:\n",
        "    g['lr'] = g['lr']*1.005\n",
        "    g['lr'] = min(g['lr'], .12)\n",
        "\n",
        "  # ログ表示\n",
        "  if iteration % 50 == 0:\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # 生成画像の表示・保存\n",
        "      out = unmap_pixels(torch.sigmoid(dec(latent())[:, :3]).float())  ###\n",
        "      displ(out.cpu()[0])  ###\n",
        "\n",
        "      # データ表示\n",
        "      print('iter = ',iteration)\n",
        "      for g in optimizer.param_groups:\n",
        "        print('lr = ', g['lr'])\n",
        "      print('tau_value = ', tau_value)\n",
        "      print('loss = ',loss.item())\n",
        "      print('\\n')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
